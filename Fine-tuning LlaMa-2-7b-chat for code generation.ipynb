{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate\n!pip install -q peft\n!pip install -q transformers\n!pip install -q trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-10T04:27:48.430457Z","iopub.execute_input":"2024-10-10T04:27:48.431497Z","iopub.status.idle":"2024-10-10T04:28:37.814207Z","shell.execute_reply.started":"2024-10-10T04:27:48.431454Z","shell.execute_reply":"2024-10-10T04:28:37.813257Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:28:46.173039Z","iopub.execute_input":"2024-10-10T04:28:46.173870Z","iopub.status.idle":"2024-10-10T04:28:46.180052Z","shell.execute_reply.started":"2024-10-10T04:28:46.173829Z","shell.execute_reply":"2024-10-10T04:28:46.179161Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"nikhiljatiwal/minipython-Alpaca-14k\"\n\n# Fine-tuned model name\nnew_model = \"/kaggle/working/llama-2-7b-codeAlpaca\"","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:28:48.744973Z","iopub.execute_input":"2024-10-10T04:28:48.745863Z","iopub.status.idle":"2024-10-10T04:28:48.751294Z","shell.execute_reply.started":"2024-10-10T04:28:48.745821Z","shell.execute_reply":"2024-10-10T04:28:48.750352Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:28:51.137168Z","iopub.execute_input":"2024-10-10T04:28:51.137545Z","iopub.status.idle":"2024-10-10T04:28:51.143730Z","shell.execute_reply.started":"2024-10-10T04:28:51.137511Z","shell.execute_reply":"2024-10-10T04:28:51.142729Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# ################################################################################\n# # bitsandbytes parameters\n# ################################################################################\n\n# # Activate 4-bit precision base model loading\n# use_4bit = True\n\n# # Compute dtype for 4-bit base models\n# bnb_4bit_compute_dtype = \"float16\"\n\n# # Quantization type (fp4 or nf4)\n# bnb_4bit_quant_type = \"nf4\"\n\n# # Activate nested quantization for 4-bit base models (double quantization)\n# use_nested_quant = False","metadata":{"execution":{"iopub.status.busy":"2024-10-09T20:56:01.718142Z","iopub.execute_input":"2024-10-09T20:56:01.719039Z","iopub.status.idle":"2024-10-09T20:56:01.723720Z","shell.execute_reply.started":"2024-10-09T20:56:01.719000Z","shell.execute_reply":"2024-10-09T20:56:01.722769Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"/kaggle/working/llama-2-7b-codeAlpaca\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16 training (set to True for mixed precision training)\nfp16 = True\n\n# Batch size per GPU for training\nper_device_train_batch_size = 8\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 8\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 2\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient norm (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"adamw_torch\"\n\n# Learning rate schedule\nlr_scheduler_type = \"constant\"\n\n# Group sequences into batches with the same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Ratio of steps for a linear warmup\nwarmup_ratio = 0.03\n\n# Save checkpoint every X updates steps\nsave_steps = 100\n\n# Log every X updates steps\nlogging_steps = 10","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:28:54.602803Z","iopub.execute_input":"2024-10-10T04:28:54.603298Z","iopub.status.idle":"2024-10-10T04:28:54.612278Z","shell.execute_reply.started":"2024-10-10T04:28:54.603253Z","shell.execute_reply":"2024-10-10T04:28:54.611195Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(\"PyTorch Version:\", torch.__version__)\nprint(\"CUDA Version:\", torch.version.cuda)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:14:29.846923Z","iopub.execute_input":"2024-10-09T22:14:29.847859Z","iopub.status.idle":"2024-10-09T22:14:29.853284Z","shell.execute_reply.started":"2024-10-09T22:14:29.847817Z","shell.execute_reply":"2024-10-09T22:14:29.852128Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"PyTorch Version: 2.4.0\nCUDA Version: 12.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:14:31.408978Z","iopub.execute_input":"2024-10-09T22:14:31.409671Z","iopub.status.idle":"2024-10-09T22:14:32.516733Z","shell.execute_reply.started":"2024-10-09T22:14:31.409630Z","shell.execute_reply":"2024-10-09T22:14:32.515604Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Wed Oct  9 22:14:32 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:30:12.045759Z","iopub.execute_input":"2024-10-10T04:30:12.046570Z","iopub.status.idle":"2024-10-10T04:30:12.052007Z","shell.execute_reply.started":"2024-10-10T04:30:12.046531Z","shell.execute_reply":"2024-10-10T04:30:12.051043Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load dataset\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load base model with 8-bit quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# Prepare model for training\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:14:36.072694Z","iopub.execute_input":"2024-10-09T22:14:36.073776Z","iopub.status.idle":"2024-10-09T22:15:46.472456Z","shell.execute_reply.started":"2024-10-09T22:14:36.073721Z","shell.execute_reply":"2024-10-09T22:15:46.471423Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92338873a5404ca5b058f2ea71bc421a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/25.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ecd095e3ac443f4a277a29797d87d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb727e8d2b54a7aa872ebce960628d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdfd757afbc64a809072cb9025b71995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8119ad272e1245c4ab3efe6a27c7613e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6d391c5d116459790bc6683116547d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce983482223e4944a9c446b3baaa8d90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e69479aa28249fb9fe2394a9dd1d523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0bb5bc09b064da9aaf571118a0ec897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf07394bd364b35b61b4e820dfe3ba9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7593162adad546448a23b815f7e02668"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993856ce77444cac978fa578e06555d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"156af546bafb4efeab9b8ab4eee261a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f97f5b4f394095b9b8d3165e8734dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4e91a91907443bb5b50be544bfc6ab"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import get_peft_model","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:30:19.277709Z","iopub.execute_input":"2024-10-10T04:30:19.278572Z","iopub.status.idle":"2024-10-10T04:30:19.283900Z","shell.execute_reply.started":"2024-10-10T04:30:19.278531Z","shell.execute_reply":"2024-10-10T04:30:19.282876Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, peft_config)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    max_grad_norm=max_grad_norm,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:15:57.690157Z","iopub.execute_input":"2024-10-09T22:15:57.690889Z","iopub.status.idle":"2024-10-09T22:16:12.477252Z","shell.execute_reply.started":"2024-10-09T22:15:57.690851Z","shell.execute_reply":"2024-10-09T22:16:12.476257Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38b9772afdc4cf794e9ffb612153727"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:16:19.459428Z","iopub.execute_input":"2024-10-09T22:16:19.460134Z","iopub.status.idle":"2024-10-10T03:19:39.768512Z","shell.execute_reply.started":"2024-10-09T22:16:19.460086Z","shell.execute_reply":"2024-10-10T03:19:39.767691Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113131322223227, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c80fa8e7bf04f1586f415d754f211f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241009_221642-sh9lub0x</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/worknikhilwork-self/huggingface/runs/sh9lub0x' target=\"_blank\">/kaggle/working/llama-2-7b-codeAlpaca</a></strong> to <a href='https://wandb.ai/worknikhilwork-self/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/worknikhilwork-self/huggingface' target=\"_blank\">https://wandb.ai/worknikhilwork-self/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/worknikhilwork-self/huggingface/runs/sh9lub0x' target=\"_blank\">https://wandb.ai/worknikhilwork-self/huggingface/runs/sh9lub0x</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='875' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [875/875 5:02:16, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.096200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.949600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.913400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.833700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.833100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.934800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.813200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.789500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.806700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.807800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.901000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.829900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.794300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.745100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.770700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.848200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.785100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.796800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.766500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.752600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.868400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.791100</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.741600</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.758000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.766200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.837800</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.774300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.787800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.728000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.756300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.796000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.780700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.777800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.721300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.748100</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.827200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.819000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.747300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.738000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.731500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.864200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.776200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.745400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.674100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.703400</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.834300</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.763100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.708100</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.706900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.738000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.873700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.771500</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.733100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.686700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.702400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.852800</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.760900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.742300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.648300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.728200</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.833700</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.782100</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.787000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.641500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.745500</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.871800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.784500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.762800</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.681600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.714200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.817600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.754500</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.732400</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.667400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.705700</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.836300</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.743700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.726000</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.692200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.765300</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.848700</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.755800</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.747200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.680600</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.740400</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.778400</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.682900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Run text generation pipeline with the fine-tuned model\nprompt = \"How can I write a Python program that calculates the mean, standard deviation, and coefficient of variation of a dataset from a CSV file?\"\npipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=400)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-10-10T03:51:12.422617Z","iopub.execute_input":"2024-10-10T03:51:12.423015Z","iopub.status.idle":"2024-10-10T03:52:21.808630Z","shell.execute_reply.started":"2024-10-10T03:51:12.422976Z","shell.execute_reply":"2024-10-10T03:52:21.807555Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] How can I write a Python program that calculates the mean, standard deviation, and coefficient of variation of a dataset from a CSV file? [/INST] You can use the following code to calculate the mean, standard deviation, and coefficient of variation of a dataset from a CSV file:\n\n```python\nimport pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean\nmean = df.mean()\n\n# Calculate the standard deviation\nstd = df.std()\n\n# Calculate the coefficient of variation\ncv = df.div(df.mean())\n\nprint(\"Mean:\", mean)\nprint(\"Standard Deviation:\", std)\nprint(\"Coefficient of Variation:\", cv)\n```\n\nIn this code, we first import the pandas library to load the CSV file. We then use the `read_csv()` function to load the data from the CSV file into a pandas DataFrame.\n\nNext, we calculate the mean by using the `mean()` function of the DataFrame. We calculate the standard deviation by using the `std()` function of the DataFrame.\n\nFinally, we calculate the coefficient of variation by dividing the DataFrame by its mean using the `div()` function. The coefficient of variation is then printed to the console.\n\nBy running this code, you will get the mean, standard deviation, and coefficient of variation of the dataset from the CSV file. You can modify the CSV file path to match your own dataset.\n\nNote: This code assumes that the CSV file contains numerical data. If the CSV file contains non-numerical data, you may need to modify the code accordingly. Additionally, the CSV file should have a header row containing the column names. If the CSV file does not have a header row, you may need to add\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:02:53.071555Z","iopub.execute_input":"2024-10-10T04:02:53.072429Z","iopub.status.idle":"2024-10-10T04:02:53.187044Z","shell.execute_reply.started":"2024-10-10T04:02:53.072378Z","shell.execute_reply":"2024-10-10T04:02:53.186328Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Empty VRAM\n# del model\n# del pipe\n# del trainer\n# del dataset\ndel tokenizer\nimport gc\ngc.collect()\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:39:08.011127Z","iopub.execute_input":"2024-10-10T04:39:08.011530Z","iopub.status.idle":"2024-10-10T04:39:08.823692Z","shell.execute_reply.started":"2024-10-10T04:39:08.011493Z","shell.execute_reply":"2024-10-10T04:39:08.822729Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Check the number of GPUs available\nnum_gpus = torch.cuda.device_count()\nprint(f\"Number of GPUs available: {num_gpus}\")\n\n# Check if CUDA device 1 is available\nif num_gpus > 1:\n    print(\"cuda:1 is available.\")\nelse:\n    print(\"cuda:1 is not available.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:43:01.022838Z","iopub.execute_input":"2024-10-10T04:43:01.024210Z","iopub.status.idle":"2024-10-10T04:43:01.032322Z","shell.execute_reply.started":"2024-10-10T04:43:01.024148Z","shell.execute_reply":"2024-10-10T04:43:01.031111Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Number of GPUs available: 2\ncuda:1 is available.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Specify the device ID for your desired GPU (e.g., 0 for the first GPU, 1 for the second GPU)\ndevice_id = 1  # Change this based on your available GPUs\ndevice = f\"cuda:{device_id}\"\n\n# Load the base model on the specified GPU\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",  # Use auto to load on the available device\n)\n\n# Load the LoRA weights\nlora_model = PeftModel.from_pretrained(base_model, new_model)\n\n# Move LoRA model to the specified GPU\nlora_model.to(device)\n\n# Merge the LoRA weights with the base model weights\nmodel = lora_model.merge_and_unload()\n\n# Ensure the merged model is on the correct device\nmodel.to(device)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-10-10T04:44:05.748951Z","iopub.execute_input":"2024-10-10T04:44:05.749851Z","iopub.status.idle":"2024-10-10T04:44:12.172850Z","shell.execute_reply.started":"2024-10-10T04:44:05.749812Z","shell.execute_reply":"2024-10-10T04:44:12.172079Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0daa37b304844210bc3343bf9b284a3b"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub('nikhiljatiwal/llama-2-7b-codeAlpaca', use_auth_token=secret_value_0)\ntokenizer.push_to_hub('nikhiljatiwal/llama-2-7b-codeAlpaca', use_auth_token=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:02:21.501511Z","iopub.execute_input":"2024-10-10T05:02:21.501905Z","iopub.status.idle":"2024-10-10T05:06:19.372449Z","shell.execute_reply.started":"2024-10-10T05:02:21.501868Z","shell.execute_reply":"2024-10-10T05:06:19.371015Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d728e925db64887b9beff374374590a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5141290b227547a79e4f55ffce0acbe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7f648dd230845c1bb4ae441f60e4c56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbcc4f8e08f44939396117b9fd0e91a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c4ea8e637243908aafe3fa86f194ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae7ab4c5aa744c6b16a1ba7cad6f2e5"}},"metadata":{}},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/nikhiljatiwal/llama-2-7b-codeAlpaca/commit/17b0f3143731200740c684da5d0ae0b05121b31a', commit_message='Upload tokenizer', commit_description='', oid='17b0f3143731200740c684da5d0ae0b05121b31a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/nikhiljatiwal/llama-2-7b-codeAlpaca', endpoint='https://huggingface.co', repo_type='model', repo_id='nikhiljatiwal/llama-2-7b-codeAlpaca'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}